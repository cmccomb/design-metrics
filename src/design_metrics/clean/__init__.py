"""Integrity checks and cleaning utilities for the minimal bibliometric schema."""

from __future__ import annotations

import re
from collections.abc import Iterable, Mapping, MutableSequence, Sequence
from dataclasses import dataclass, field
from difflib import SequenceMatcher

import numpy as np
import pandas as pd

_REQUIRED_TABLES = {
    "papers": {"paper_id", "title", "year"},
    "authors": {"author_id", "name"},
    "authorships": {"paper_id", "author_id"},
}

_OPTIONAL_TABLES = {
    "institutions": {"institution_id", "name", "country"},
    "affiliations": {"paper_id", "author_id", "institution_id"},
    "references": {"citing_paper_id", "cited_text"},
}


@dataclass
class ValidationIssue:
    """An individual schema or data quality issue."""

    table: str
    message: str


@dataclass
class ValidationReport:
    """Structured report generated by :func:`validate_schema`."""

    issues: MutableSequence[ValidationIssue] = field(default_factory=list)

    @property
    def ok(self) -> bool:
        """Return ``True`` when no issues were detected."""

        return not self.issues

    def to_frame(self) -> pd.DataFrame:
        """Return issues as a DataFrame for logging or display."""

        if not self.issues:
            return pd.DataFrame(columns=["table", "message"])
        return pd.DataFrame([issue.__dict__ for issue in self.issues])


def validate_schema(tables: Mapping[str, pd.DataFrame]) -> ValidationReport:
    """Validate that provided tables satisfy the minimal schema."""

    report = ValidationReport()

    for table, required_columns in _REQUIRED_TABLES.items():
        if table not in tables:
            report.issues.append(ValidationIssue(table, "missing required table"))
            continue
        _validate_required_columns(tables[table], required_columns, table, report)
        _validate_nulls(tables[table], required_columns, table, report)

    for table, required_columns in _OPTIONAL_TABLES.items():
        if table in tables:
            _validate_required_columns(tables[table], required_columns, table, report)

    if "papers" in tables and "paper_id" in tables["papers"].columns:
        duplicates = tables["papers"]["paper_id"].duplicated()
        if duplicates.any():
            report.issues.append(
                ValidationIssue("papers", "duplicate paper_id values detected")
            )

    if "authors" in tables and "author_id" in tables["authors"].columns:
        duplicates = tables["authors"]["author_id"].duplicated()
        if duplicates.any():
            report.issues.append(
                ValidationIssue("authors", "duplicate author_id values detected")
            )

    return report


def dedupe_papers(
    papers: pd.DataFrame,
    *,
    by: Sequence[str] | None = None,
    similarity: float = 1.0,
) -> pd.DataFrame:
    """Remove likely duplicate paper rows using exact or fuzzy matching."""

    if not 0.0 < similarity <= 1.0:
        raise ValueError("similarity must be within (0, 1]")

    columns = list(by) if by is not None else ["title", "year"]
    missing = [column for column in columns if column not in papers.columns]
    if missing:
        raise ValueError(f"Columns missing from papers DataFrame: {missing}")

    if similarity >= 0.9999:
        return papers.drop_duplicates(subset=columns).reset_index(drop=True)

    normalised = papers.copy()
    for column in columns:
        if _looks_like_string(normalised[column]):
            normalised[column] = normalised[column].map(_normalise_text)

    keep_mask = np.ones(len(normalised), dtype=bool)
    for idx in range(len(normalised)):
        if not keep_mask[idx]:
            continue
        for jdx in range(idx + 1, len(normalised)):
            if not keep_mask[jdx]:
                continue
            similarity_score = _row_similarity(
                normalised.iloc[idx], normalised.iloc[jdx], columns
            )
            if similarity_score >= similarity:
                keep_mask[jdx] = False
    return papers.loc[keep_mask].reset_index(drop=True)


def normalize_authors(
    authors: pd.DataFrame,
    *,
    strategy: str = "lastname_initials",
    name_column: str = "name",
) -> pd.DataFrame:
    """Normalise author names according to a chosen strategy."""

    if name_column not in authors.columns:
        raise ValueError(f"Column '{name_column}' not present in authors DataFrame")

    frame = authors.copy()
    strategy_lower = strategy.lower()
    if strategy_lower == "lastname_initials":
        frame[name_column] = frame[name_column].map(_format_lastname_initials)
    elif strategy_lower == "simple":
        frame[name_column] = frame[name_column].map(_simple_title_case)
    else:
        raise ValueError(
            "Unsupported strategy. Choose from 'lastname_initials' or 'simple'."
        )

    frame["name_sort_key"] = frame[name_column].map(_normalise_text)
    return frame.reset_index(drop=True)


def _validate_required_columns(
    frame: pd.DataFrame,
    required: Iterable[str],
    table: str,
    report: ValidationReport,
) -> None:
    missing = [column for column in required if column not in frame.columns]
    if missing:
        report.issues.append(
            ValidationIssue(table, f"missing required columns: {', '.join(missing)}")
        )


def _validate_nulls(
    frame: pd.DataFrame,
    required: Iterable[str],
    table: str,
    report: ValidationReport,
) -> None:
    for column in required:
        if column in frame.columns and frame[column].isna().any():
            report.issues.append(
                ValidationIssue(
                    table, f"null values present in required column '{column}'"
                )
            )


def _row_similarity(
    row_a: pd.Series, row_b: pd.Series, columns: Sequence[str]
) -> float:
    scores: MutableSequence[float] = []
    for column in columns:
        value_a = row_a[column]
        value_b = row_b[column]
        if pd.isna(value_a) or pd.isna(value_b):
            continue
        if isinstance(value_a, str) or isinstance(value_b, str):
            scores.append(_text_similarity(str(value_a), str(value_b)))
        else:
            scores.append(1.0 if value_a == value_b else 0.0)
    return float(np.mean(scores)) if scores else 0.0


def _text_similarity(value_a: str, value_b: str) -> float:
    return SequenceMatcher(None, value_a, value_b).ratio()


def _looks_like_string(series: pd.Series) -> bool:
    dtype = series.dtype
    return bool(
        pd.api.types.is_string_dtype(dtype) or pd.api.types.is_object_dtype(dtype)
    )


def _format_lastname_initials(value: object) -> str:
    parts = _tokenise_name(value)
    if not parts:
        return ""
    *given_names, surname = parts
    initials = " ".join(f"{name[0].upper()}." for name in given_names if name)
    if initials:
        return f"{surname.title()}, {initials}".strip()
    return surname.title()


def _simple_title_case(value: object) -> str:
    text = str(value).strip()
    text = " ".join(token for token in text.split())
    return text.title()


def _tokenise_name(value: object) -> list[str]:
    text = _normalise_text(value)
    return [token for token in text.split() if token]


def _normalise_text(value: object) -> str:
    text = str(value).casefold()
    text = re.sub(r"[^a-z0-9]+", " ", text)
    return text.strip()


__all__ = [
    "ValidationIssue",
    "ValidationReport",
    "validate_schema",
    "dedupe_papers",
    "normalize_authors",
    "_normalise_text",
]
